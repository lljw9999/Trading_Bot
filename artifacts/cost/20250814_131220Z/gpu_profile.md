# GPU Performance Profile Report

**Generated:** 2025-08-14T13:12:21.063901Z  
**Duration:** 5s

## GPU Hardware Status

| Metric | Value |
|--------|-------|
| **GPU** | NVIDIA RTX 4090 |
| **Memory Usage** | 20.4 / 24.0 GB (85%) |
| **GPU Utilization** | 78% |
| **Temperature** | 72Â°C |
| **Power Draw** | 220W |

## Performance Benchmarks

### Optimal Configuration
- **Best Batch Size:** 16
- **Peak Throughput:** 2,667 inferences/sec
- **P95 Latency:** 8.4ms
- **Memory Usage:** 896 MB
- **Efficiency Score:** 2976.2

### Batch Size Analysis

| Batch | Latency (P95) | Throughput | Memory | Efficiency |
|-------|---------------|------------|--------|-----------|
| 1 | 33.6ms | 42/s | 536MB | 77.7 |
| 4 | 13.4ms | 417/s | 608MB | 685.3 |
| 8 | 10.1ms | 1,111/s | 704MB | 1578.3 |
| 16 | 8.4ms | 2,667/s | 896MB | 2976.2 |
| 32 | 15.1ms | 2,963/s | 1280MB | 2314.8 |
| 64 | 28.6ms | 3,137/s | 2048MB | 1531.9 |


## Compute Utilization

- **Estimated TFLOPs:** 0.95 / 83.0 peak
- **Compute Utilization:** 1.1%
- **Model Parameters:** 125M

## Optimization Recommendations

### ðŸŸ¡ Compute - MEDIUM Priority
- **Issue:** Low GPU compute utilization (1.1%) - batching or model complexity could be improved
- **Action:** `optimize_batching`

### ðŸŸ¡ Cost - MEDIUM Priority
- **Issue:** High power draw - consider power limiting or efficiency optimizations
- **Action:** `power_optimization`



## Next Steps

1. **Immediate:** Configure production inference with batch size **16**
2. **Short-term:** Implement quantization to achieve >1.5x throughput improvement
3. **Long-term:** Monitor GPU utilization trends and right-size instances

---
*Report generated by GPU Profiler - Cost & Efficiency Program M10*
